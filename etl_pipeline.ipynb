{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c279909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-4.0.1.tar.gz (434.2 MB)\n",
      "     ---------------------------------------- 0.0/434.2 MB ? eta -:--:--\n",
      "     --------------------------------------- 1.0/434.2 MB 13.1 MB/s eta 0:00:34\n",
      "     --------------------------------------- 3.7/434.2 MB 12.7 MB/s eta 0:00:34\n",
      "      -------------------------------------- 6.3/434.2 MB 13.0 MB/s eta 0:00:33\n",
      "      ------------------------------------- 10.0/434.2 MB 13.9 MB/s eta 0:00:31\n",
      "     - ------------------------------------ 12.3/434.2 MB 13.6 MB/s eta 0:00:32\n",
      "     - ------------------------------------ 15.2/434.2 MB 13.9 MB/s eta 0:00:31\n",
      "     - ------------------------------------ 16.5/434.2 MB 12.5 MB/s eta 0:00:34\n",
      "     - ------------------------------------ 19.9/434.2 MB 12.9 MB/s eta 0:00:33\n",
      "     -- ----------------------------------- 24.1/434.2 MB 13.6 MB/s eta 0:00:31\n",
      "     -- ----------------------------------- 28.0/434.2 MB 14.1 MB/s eta 0:00:29\n",
      "     -- ----------------------------------- 31.7/434.2 MB 14.5 MB/s eta 0:00:28\n",
      "     -- ----------------------------------- 33.8/434.2 MB 14.1 MB/s eta 0:00:29\n",
      "     --- ---------------------------------- 37.0/434.2 MB 14.1 MB/s eta 0:00:29\n",
      "     --- ---------------------------------- 41.4/434.2 MB 14.7 MB/s eta 0:00:27\n",
      "     ---- --------------------------------- 45.9/434.2 MB 15.1 MB/s eta 0:00:26\n",
      "     ---- --------------------------------- 50.6/434.2 MB 15.5 MB/s eta 0:00:25\n",
      "     ---- --------------------------------- 55.1/434.2 MB 15.8 MB/s eta 0:00:24\n",
      "     ----- -------------------------------- 60.6/434.2 MB 16.4 MB/s eta 0:00:23\n",
      "     ----- -------------------------------- 65.8/434.2 MB 16.8 MB/s eta 0:00:22\n",
      "     ------ ------------------------------- 69.7/434.2 MB 16.9 MB/s eta 0:00:22\n",
      "     ------ ------------------------------- 74.2/434.2 MB 17.1 MB/s eta 0:00:22\n",
      "     ------ ------------------------------- 78.9/434.2 MB 17.3 MB/s eta 0:00:21\n",
      "     ------- ------------------------------ 83.4/434.2 MB 17.6 MB/s eta 0:00:20\n",
      "     ------- ------------------------------ 88.1/434.2 MB 17.8 MB/s eta 0:00:20\n",
      "     -------- ----------------------------- 93.1/434.2 MB 18.0 MB/s eta 0:00:19\n",
      "     -------- ----------------------------- 98.6/434.2 MB 18.3 MB/s eta 0:00:19\n",
      "     -------- ---------------------------- 102.8/434.2 MB 18.4 MB/s eta 0:00:19\n",
      "     --------- --------------------------- 108.3/434.2 MB 18.6 MB/s eta 0:00:18\n",
      "     --------- --------------------------- 113.2/434.2 MB 18.8 MB/s eta 0:00:18\n",
      "     ---------- -------------------------- 118.0/434.2 MB 18.9 MB/s eta 0:00:17\n",
      "     ---------- -------------------------- 122.2/434.2 MB 18.9 MB/s eta 0:00:17\n",
      "     ---------- -------------------------- 125.8/434.2 MB 18.9 MB/s eta 0:00:17\n",
      "     ----------- ------------------------- 130.8/434.2 MB 19.0 MB/s eta 0:00:16\n",
      "     ----------- ------------------------- 136.3/434.2 MB 19.2 MB/s eta 0:00:16\n",
      "     ------------ ------------------------ 141.8/434.2 MB 19.4 MB/s eta 0:00:16\n",
      "     ------------ ------------------------ 146.8/434.2 MB 19.6 MB/s eta 0:00:15\n",
      "     ------------- ----------------------- 152.6/434.2 MB 19.7 MB/s eta 0:00:15\n",
      "     ------------- ----------------------- 157.8/434.2 MB 19.8 MB/s eta 0:00:14\n",
      "     ------------- ----------------------- 162.5/434.2 MB 19.9 MB/s eta 0:00:14\n",
      "     -------------- ---------------------- 167.8/434.2 MB 20.0 MB/s eta 0:00:14\n",
      "     -------------- ---------------------- 173.0/434.2 MB 20.1 MB/s eta 0:00:14\n",
      "     --------------- --------------------- 178.0/434.2 MB 20.2 MB/s eta 0:00:13\n",
      "     --------------- --------------------- 183.0/434.2 MB 20.2 MB/s eta 0:00:13\n",
      "     ---------------- -------------------- 188.0/434.2 MB 20.3 MB/s eta 0:00:13\n",
      "     ---------------- -------------------- 192.9/434.2 MB 20.4 MB/s eta 0:00:12\n",
      "     ---------------- -------------------- 197.4/434.2 MB 20.4 MB/s eta 0:00:12\n",
      "     ----------------- ------------------- 202.4/434.2 MB 20.5 MB/s eta 0:00:12\n",
      "     ----------------- ------------------- 206.8/434.2 MB 20.5 MB/s eta 0:00:12\n",
      "     ------------------ ------------------ 211.6/434.2 MB 20.6 MB/s eta 0:00:11\n",
      "     ------------------ ------------------ 217.1/434.2 MB 20.7 MB/s eta 0:00:11\n",
      "     ------------------ ------------------ 222.0/434.2 MB 20.7 MB/s eta 0:00:11\n",
      "     ------------------- ----------------- 227.0/434.2 MB 20.8 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 231.5/434.2 MB 20.8 MB/s eta 0:00:10\n",
      "     -------------------- ---------------- 236.5/434.2 MB 20.9 MB/s eta 0:00:10\n",
      "     -------------------- ---------------- 241.4/434.2 MB 20.9 MB/s eta 0:00:10\n",
      "     --------------------- --------------- 246.7/434.2 MB 21.0 MB/s eta 0:00:09\n",
      "     --------------------- --------------- 251.9/434.2 MB 21.1 MB/s eta 0:00:09\n",
      "     --------------------- --------------- 257.4/434.2 MB 21.1 MB/s eta 0:00:09\n",
      "     ---------------------- -------------- 262.4/434.2 MB 21.2 MB/s eta 0:00:09\n",
      "     ---------------------- -------------- 268.2/434.2 MB 21.6 MB/s eta 0:00:08\n",
      "     ----------------------- ------------- 272.9/434.2 MB 21.8 MB/s eta 0:00:08\n",
      "     ----------------------- ------------- 277.9/434.2 MB 22.2 MB/s eta 0:00:08\n",
      "     ------------------------ ------------ 282.3/434.2 MB 22.5 MB/s eta 0:00:07\n",
      "     ------------------------ ------------ 287.0/434.2 MB 22.5 MB/s eta 0:00:07\n",
      "     ------------------------ ------------ 291.2/434.2 MB 22.6 MB/s eta 0:00:07\n",
      "     ------------------------- ----------- 296.7/434.2 MB 23.0 MB/s eta 0:00:06\n",
      "     ------------------------- ----------- 301.2/434.2 MB 23.0 MB/s eta 0:00:06\n",
      "     -------------------------- ---------- 306.2/434.2 MB 23.1 MB/s eta 0:00:06\n",
      "     -------------------------- ---------- 310.6/434.2 MB 23.1 MB/s eta 0:00:06\n",
      "     -------------------------- ---------- 315.4/434.2 MB 23.1 MB/s eta 0:00:06\n",
      "     --------------------------- --------- 319.3/434.2 MB 23.1 MB/s eta 0:00:05\n",
      "     --------------------------- --------- 323.2/434.2 MB 22.9 MB/s eta 0:00:05\n",
      "     --------------------------- --------- 327.9/434.2 MB 22.9 MB/s eta 0:00:05\n",
      "     ---------------------------- -------- 332.4/434.2 MB 22.9 MB/s eta 0:00:05\n",
      "     ---------------------------- -------- 336.9/434.2 MB 23.0 MB/s eta 0:00:05\n",
      "     ----------------------------- ------- 342.1/434.2 MB 23.0 MB/s eta 0:00:05\n",
      "     ----------------------------- ------- 346.8/434.2 MB 23.0 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 351.0/434.2 MB 22.9 MB/s eta 0:00:04\n",
      "     ------------------------------ ------ 355.7/434.2 MB 22.9 MB/s eta 0:00:04\n",
      "     ------------------------------ ------ 360.4/434.2 MB 22.9 MB/s eta 0:00:04\n",
      "     ------------------------------- ----- 365.7/434.2 MB 22.9 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 370.7/434.2 MB 22.9 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 374.9/434.2 MB 22.8 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 379.8/434.2 MB 22.9 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 384.3/434.2 MB 22.9 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 389.8/434.2 MB 23.1 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 394.3/434.2 MB 23.0 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 399.0/434.2 MB 22.9 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 403.7/434.2 MB 22.9 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 408.4/434.2 MB 22.8 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 413.7/434.2 MB 22.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 417.6/434.2 MB 22.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 422.3/434.2 MB 22.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  426.2/434.2 MB 22.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  431.2/434.2 MB 22.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.2 MB 22.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.2 MB 22.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.2 MB 22.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 434.2/434.2 MB 21.5 MB/s  0:00:20\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.3-cp314-cp314-win_amd64.whl.metadata (19 kB)\n",
      "Collecting sqlalchemy\n",
      "  Using cached sqlalchemy-2.0.45-cp314-cp314-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting psycopg2-binary\n",
      "  Using cached psycopg2_binary-2.9.11-cp314-cp314-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting py4j==0.10.9.9 (from pyspark)\n",
      "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.3.5-cp314-cp314-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\onyek\\documents\\10alytics training\\flour4fourpyspark\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy)\n",
      "  Using cached greenlet-3.3.0-cp314-cp314-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting typing-extensions>=4.6.0 (from sqlalchemy)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\onyek\\documents\\10alytics training\\flour4fourpyspark\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Using cached pandas-2.3.3-cp314-cp314-win_amd64.whl (11.1 MB)\n",
      "Using cached sqlalchemy-2.0.45-cp314-cp314-win_amd64.whl (2.1 MB)\n",
      "Using cached psycopg2_binary-2.9.11-cp314-cp314-win_amd64.whl (2.8 MB)\n",
      "Using cached greenlet-3.3.0-cp314-cp314-win_amd64.whl (305 kB)\n",
      "Using cached numpy-2.3.5-cp314-cp314-win_amd64.whl (12.9 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml): started\n",
      "  Building wheel for pyspark (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-4.0.1-py2.py3-none-any.whl size=434903354 sha256=8da2c2488dcd997230900bd6d0b11b11a1899b42991bd25c8a8425600a0c462c\n",
      "  Stored in directory: c:\\users\\onyek\\appdata\\local\\pip\\cache\\wheels\\a1\\70\\d4\\2677f8b10c903e8bed0e828f10297861ee280649b88266b047\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pytz, py4j, tzdata, typing-extensions, pyspark, psycopg2-binary, numpy, greenlet, sqlalchemy, pandas\n",
      "\n",
      "   ----------------------------------------  0/10 [pytz]\n",
      "   ---- -----------------------------------  1/10 [py4j]\n",
      "   -------- -------------------------------  2/10 [tzdata]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ---------------- -----------------------  4/10 [pyspark]\n",
      "   ------------------------ ---------------  6/10 [numpy]\n",
      "   ------------------------ ---------------  6/10 [numpy]\n",
      "   ------------------------ ---------------  6/10 [numpy]\n",
      "   ------------------------ ---------------  6/10 [numpy]\n",
      "   ------------------------ ---------------  6/10 [numpy]\n",
      "   ------------------------ ---------------  6/10 [numpy]\n",
      "   ------------------------ ---------------  6/10 [numpy]\n",
      "   ------------------------ ---------------  6/10 [numpy]\n",
      "   ------------------------ ---------------  6/10 [numpy]\n",
      "   ------------------------ ---------------  6/10 [numpy]\n",
      "   ------------------------ ---------------  6/10 [numpy]\n",
      "   -------------------------------- -------  8/10 [sqlalchemy]\n",
      "   -------------------------------- -------  8/10 [sqlalchemy]\n",
      "   -------------------------------- -------  8/10 [sqlalchemy]\n",
      "   -------------------------------- -------  8/10 [sqlalchemy]\n",
      "   -------------------------------- -------  8/10 [sqlalchemy]\n",
      "   -------------------------------- -------  8/10 [sqlalchemy]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ------------------------------------ ---  9/10 [pandas]\n",
      "   ---------------------------------------- 10/10 [pandas]\n",
      "\n",
      "Successfully installed greenlet-3.3.0 numpy-2.3.5 pandas-2.3.3 psycopg2-binary-2.9.11 py4j-0.10.9.9 pyspark-4.0.1 pytz-2025.2 sqlalchemy-2.0.45 typing-extensions-4.15.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark pandas sqlalchemy psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f800b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Necessary Libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrameWriter\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b61267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Flour4Four_ETL_PySpark\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2067ac08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Flour4Four_ETL_PySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x17c7a6696a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e278b04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+-----------+--------------------+-------------+--------------------+---------------+-------------+------------+-------------+-------------+------------+--------------+------------+-------------+-----------+\n",
      "|  order_id|order_date|delivery_date|business_id|       business_name|business_type|    business_address|   contact_name|contact_phone|  flour_type|quantity_bags|price_per_bag|total_amount|payment_method|order_status|   rider_name|rider_phone|\n",
      "+----------+----------+-------------+-----------+--------------------+-------------+--------------------+---------------+-------------+------------+-------------+-------------+------------+--------------+------------+-------------+-----------+\n",
      "|ORD-214576|2025-10-25|   2025-10-25|   BIZ-1018|Adams, Zuniga and...|   Restaurant|Herbert Macaulay ...|   Elimu Agbaje|   8017507864|        NULL|           26|         9500|      247000|           POS|   Delivered|  Aisha Bello| 8089864260|\n",
      "|ORD-299448|2025-10-08|   2025-10-08|   BIZ-1006|      Blake and Sons|       Bakery|Ahmadu Bello Way,...|Bolanle Kalumba|   8055667651| Bread Flour|           27|        10000|      270000|           POS|   Cancelled|Tunde Oladipo| 8019121552|\n",
      "|ORD-246991|      NULL|   2025-10-17|   BIZ-1052|    Chapman and Sons|         Cafe|Ahmadu Bello Way,...|   Hassan Nyoni|   8083863413|Pastry Flour|           21|         9800|      205800| Bank Transfer|   Cancelled|Tunde Oladipo| 8019121552|\n",
      "|ORD-392075|2025-10-13|   2025-10-13|   BIZ-1035|    Rodriguez-Graham|   Restaurant|Herbert Macaulay ...|Mandela Onyango|   8075228535| All-purpose|           20|        10500|      210000| Bank Transfer|     Pending|Tunde Oladipo| 8019121552|\n",
      "|ORD-179046|2025-10-14|   2025-10-14|   BIZ-1039|Romero, Gonzalez ...|         NULL| Garki Area 1, Abuja|  Mojisola Seko|   8060119651| Bread Flour|           40|         NULL|      380000|           POS|     Pending|   Emeka John| 8019196777|\n",
      "|ORD-997546|2025-10-14|   2025-10-14|   BIZ-1025|       Wilkerson-Day|         Cafe|Ahmadu Bello Way,...|  Chioma Mensah|   8024716857| Bread Flour|            9|        10000|       90000|          Cash|     Pending|   Emeka John| 8019196777|\n",
      "|ORD-153045|2025-10-17|   2025-10-17|   BIZ-1001|           Doyle Ltd|   Restaurant|Ahmadu Bello Way,...|  Adaeze Fofana|   8083197857| All-purpose|           11|        10500|      115500|           POS|   Cancelled|Tunde Oladipo| 8019121552|\n",
      "|ORD-666847|2025-10-20|   2025-10-20|   BIZ-1004|Guzman, Hoffman a...|       Bakery|Herbert Macaulay ...|    Osman Touré|   8097226012| Whole Wheat|           20|         9500|      190000| Bank Transfer|   Delivered|  Aisha Bello| 8089864260|\n",
      "|ORD-197793|2025-10-26|   2025-10-26|   BIZ-1023|       Morales-Jones|    Fast Food|Herbert Macaulay ...| Yemisi Kalumba|   8088320463| Whole Wheat|           36|         9800|      352800|           POS|   Delivered|Tunde Oladipo| 8019121552|\n",
      "|ORD-989921|2025-10-28|   2025-10-28|   BIZ-1032|Ferrell, Rice and...|   Restaurant|Ahmadu Bello Way,...|   Leko Nnamani|   8031682744|Pastry Flour|           22|         9800|      215600|          Cash|   Delivered|Tunde Oladipo| 8019121552|\n",
      "+----------+----------+-------------+-----------+--------------------+-------------+--------------------+---------------+-------------+------------+-------------+-------------+------------+--------------+------------+-------------+-----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Data Extraction to Spark DataFrame\n",
    "f4f_df = spark.read.csv(r'raw_data\\flour4four_orders.csv', header=True, inferSchema=True, nullValue=\"None\") \n",
    "\n",
    "f4f_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d084cb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no of rows\n",
    "num_rows = f4f_df.count()\n",
    "\n",
    "num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3ea7869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no of columns \n",
    "num_columns = len(f4f_df.columns)\n",
    "\n",
    "num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1545d4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id Nulls 0\n",
      "order_date Nulls 628\n",
      "delivery_date Nulls 0\n",
      "business_id Nulls 0\n",
      "business_name Nulls 602\n",
      "business_type Nulls 987\n",
      "business_address Nulls 978\n",
      "contact_name Nulls 0\n",
      "contact_phone Nulls 0\n",
      "flour_type Nulls 642\n",
      "quantity_bags Nulls 0\n",
      "price_per_bag Nulls 636\n",
      "total_amount Nulls 0\n",
      "payment_method Nulls 0\n",
      "order_status Nulls 0\n",
      "rider_name Nulls 0\n",
      "rider_phone Nulls 0\n"
     ]
    }
   ],
   "source": [
    "# Checking for null values\n",
    "for column in f4f_df.columns:\n",
    "    print(column, 'Nulls', f4f_df.filter(f4f_df[column].isNull()).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b9f527e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+-----------+--------------------+-------------+--------------------+---------------+-------------+------------+-------------+-------------+------------+--------------+------------+-------------+-----------+\n",
      "|  order_id|order_date|delivery_date|business_id|       business_name|business_type|    business_address|   contact_name|contact_phone|  flour_type|quantity_bags|price_per_bag|total_amount|payment_method|order_status|   rider_name|rider_phone|\n",
      "+----------+----------+-------------+-----------+--------------------+-------------+--------------------+---------------+-------------+------------+-------------+-------------+------------+--------------+------------+-------------+-----------+\n",
      "|ORD-214576|2025-10-25|   2025-10-25|   BIZ-1018|Adams, Zuniga and...|   Restaurant|Herbert Macaulay ...|   Elimu Agbaje|   8017507864|        NULL|           26|         9500|      247000|           POS|   Delivered|  Aisha Bello| 8089864260|\n",
      "|ORD-299448|2025-10-08|   2025-10-08|   BIZ-1006|      Blake and Sons|       Bakery|Ahmadu Bello Way,...|Bolanle Kalumba|   8055667651| Bread Flour|           27|        10000|      270000|           POS|   Cancelled|Tunde Oladipo| 8019121552|\n",
      "|ORD-246991|      NULL|   2025-10-17|   BIZ-1052|    Chapman and Sons|         Cafe|Ahmadu Bello Way,...|   Hassan Nyoni|   8083863413|Pastry Flour|           21|         9800|      205800| Bank Transfer|   Cancelled|Tunde Oladipo| 8019121552|\n",
      "|ORD-392075|2025-10-13|   2025-10-13|   BIZ-1035|    Rodriguez-Graham|   Restaurant|Herbert Macaulay ...|Mandela Onyango|   8075228535| All-purpose|           20|        10500|      210000| Bank Transfer|     Pending|Tunde Oladipo| 8019121552|\n",
      "|ORD-179046|2025-10-14|   2025-10-14|   BIZ-1039|Romero, Gonzalez ...|         NULL| Garki Area 1, Abuja|  Mojisola Seko|   8060119651| Bread Flour|           40|         NULL|      380000|           POS|     Pending|   Emeka John| 8019196777|\n",
      "+----------+----------+-------------+-----------+--------------------+-------------+--------------------+---------------+-------------+------------+-------------+-------------+------------+--------------+------------+-------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "f4f_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f57dfa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill up the missing values\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Replace null order_date with delivery_date\n",
    "f4f_df = f4f_df.withColumn(\n",
    "    \"order_date\",\n",
    "    F.coalesce(\"order_date\", \"delivery_date\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e4a721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+-----------+--------------------+-------------+--------------------+---------------+-------------+------------+-------------+-------------+------------+--------------+------------+-------------+-----------+\n",
      "|  order_id|order_date|delivery_date|business_id|       business_name|business_type|    business_address|   contact_name|contact_phone|  flour_type|quantity_bags|price_per_bag|total_amount|payment_method|order_status|   rider_name|rider_phone|\n",
      "+----------+----------+-------------+-----------+--------------------+-------------+--------------------+---------------+-------------+------------+-------------+-------------+------------+--------------+------------+-------------+-----------+\n",
      "|ORD-214576|2025-10-25|   2025-10-25|   BIZ-1018|Adams, Zuniga and...|   Restaurant|Herbert Macaulay ...|   Elimu Agbaje|   8017507864|        NULL|           26|         9500|      247000|           POS|   Delivered|  Aisha Bello| 8089864260|\n",
      "|ORD-299448|2025-10-08|   2025-10-08|   BIZ-1006|      Blake and Sons|       Bakery|Ahmadu Bello Way,...|Bolanle Kalumba|   8055667651| Bread Flour|           27|        10000|      270000|           POS|   Cancelled|Tunde Oladipo| 8019121552|\n",
      "|ORD-246991|2025-10-17|   2025-10-17|   BIZ-1052|    Chapman and Sons|         Cafe|Ahmadu Bello Way,...|   Hassan Nyoni|   8083863413|Pastry Flour|           21|         9800|      205800| Bank Transfer|   Cancelled|Tunde Oladipo| 8019121552|\n",
      "|ORD-392075|2025-10-13|   2025-10-13|   BIZ-1035|    Rodriguez-Graham|   Restaurant|Herbert Macaulay ...|Mandela Onyango|   8075228535| All-purpose|           20|        10500|      210000| Bank Transfer|     Pending|Tunde Oladipo| 8019121552|\n",
      "|ORD-179046|2025-10-14|   2025-10-14|   BIZ-1039|Romero, Gonzalez ...|         NULL| Garki Area 1, Abuja|  Mojisola Seko|   8060119651| Bread Flour|           40|         NULL|      380000|           POS|     Pending|   Emeka John| 8019196777|\n",
      "+----------+----------+-------------+-----------+--------------------+-------------+--------------------+---------------+-------------+------------+-------------+-------------+------------+--------------+------------+-------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "f4f_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ba15aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id Nulls 0\n",
      "order_date Nulls 0\n",
      "delivery_date Nulls 0\n",
      "business_id Nulls 0\n",
      "business_name Nulls 0\n",
      "business_type Nulls 0\n",
      "business_address Nulls 0\n",
      "contact_name Nulls 0\n",
      "contact_phone Nulls 0\n",
      "flour_type Nulls 0\n",
      "quantity_bags Nulls 0\n",
      "price_per_bag Nulls 636\n",
      "total_amount Nulls 0\n",
      "payment_method Nulls 0\n",
      "order_status Nulls 0\n",
      "rider_name Nulls 0\n",
      "rider_phone Nulls 0\n"
     ]
    }
   ],
   "source": [
    "# fill up the missing values\n",
    "f4f_df_clean = f4f_df.fillna({\n",
    "    'business_name': 'Unknown',\n",
    "    'business_type': 'Unknown',\n",
    "    'business_address': 'Unknown',\n",
    "    'flour_type': 'Unknown',\n",
    "          \n",
    "})\n",
    "\n",
    "for column in f4f_df_clean.columns:\n",
    "    print(column, 'Nulls', f4f_df_clean.filter(f4f_df_clean[column].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a62a396a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- delivery_date: date (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- business_name: string (nullable = false)\n",
      " |-- business_type: string (nullable = false)\n",
      " |-- business_address: string (nullable = false)\n",
      " |-- contact_name: string (nullable = true)\n",
      " |-- contact_phone: long (nullable = true)\n",
      " |-- flour_type: string (nullable = false)\n",
      " |-- quantity_bags: integer (nullable = true)\n",
      " |-- price_per_bag: integer (nullable = true)\n",
      " |-- total_amount: integer (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- rider_name: string (nullable = true)\n",
      " |-- rider_phone: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill price_per_bag with median\n",
    "median_price = f4f_df_clean.approxQuantile(\"price_per_bag\", [0.5], 0.01)[0]\n",
    "f4f_df_clean = f4f_df_clean.na.fill({\"price_per_bag\": median_price})\n",
    "\n",
    "\n",
    "f4f_df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "219e362d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_id',\n",
       " 'order_date',\n",
       " 'delivery_date',\n",
       " 'business_id',\n",
       " 'business_name',\n",
       " 'business_type',\n",
       " 'business_address',\n",
       " 'contact_name',\n",
       " 'contact_phone',\n",
       " 'flour_type',\n",
       " 'quantity_bags',\n",
       " 'price_per_bag',\n",
       " 'total_amount',\n",
       " 'payment_method',\n",
       " 'order_status',\n",
       " 'rider_name',\n",
       " 'rider_phone']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f4f_df_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8802dd1",
   "metadata": {},
   "source": [
    "### Data Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd36275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------+--------------------+----------------+-------------+\n",
      "|business_id|       business_name|business_type|    business_address|    contact_name|contact_phone|\n",
      "+-----------+--------------------+-------------+--------------------+----------------+-------------+\n",
      "|   BIZ-1000|             Unknown|       Bakery|Ahmadu Bello Way,...|   Ijeoma Jalloh|   8042868828|\n",
      "|   BIZ-1001|           Doyle Ltd|   Restaurant|Ahmadu Bello Way,...|   Adaeze Fofana|   8083197857|\n",
      "|   BIZ-1002|Mcclain, Miller a...|       Bakery|Herbert Macaulay ...|      Asha Okeke|   8013999315|\n",
      "|   BIZ-1003|      Davis and Sons|       Bakery|Ahmadu Bello Way,...|       Zane Adom|   8090801586|\n",
      "|   BIZ-1004|Guzman, Hoffman a...|       Bakery|Herbert Macaulay ...|     Osman Touré|   8097226012|\n",
      "|   BIZ-1005|Gardner, Robinson...|         Cafe|Ahmadu Bello Way,...|  Abena Obasanjo|   8047338124|\n",
      "|   BIZ-1006|      Blake and Sons|       Bakery|Ahmadu Bello Way,...| Bolanle Kalumba|   8055667651|\n",
      "|   BIZ-1007|Henderson, Ramire...|    Fast Food|Ahmadu Bello Way,...|  Lemuel Nnamani|   8055176955|\n",
      "|   BIZ-1008|        Garcia-James|       Bakery|Ahmadu Bello Way,...| Chinonso Achebe|   8058181396|\n",
      "|   BIZ-1009|             Unknown|    Fast Food|Herbert Macaulay ...|  Dada Abdoulaye|   8071662963|\n",
      "|   BIZ-1010|             Unknown|       Bakery| Garki Area 1, Abuja| Mandela Chiluba|   8049349722|\n",
      "|   BIZ-1011|             Unknown|      Unknown|Herbert Macaulay ...|     Abdul Cissé|   8019335534|\n",
      "|   BIZ-1012|          Arnold Ltd|      Unknown|Herbert Macaulay ...|  Masamba Agbaje|   8048840994|\n",
      "|   BIZ-1013|Mcclure, Ward and...|       Bakery|Ahmadu Bello Way,...|        Aziz Eze|   8047308985|\n",
      "|   BIZ-1014|   Williams and Sons|         Cafe|Herbert Macaulay ...|      Gimba Moyo|   8059684848|\n",
      "|   BIZ-1015|      Galloway-Wyatt|    Fast Food|Ahmadu Bello Way,...|Mojisola Olamide|   8096977837|\n",
      "|   BIZ-1016|         James Group|      Unknown|Herbert Macaulay ...|  Adesuwa Chiume|   8081691040|\n",
      "|   BIZ-1017|Flowers, Martin a...|   Restaurant|Ahmadu Bello Way,...|      Jamila Eze|   8046231783|\n",
      "|   BIZ-1018|Adams, Zuniga and...|   Restaurant|Herbert Macaulay ...|    Elimu Agbaje|   8017507864|\n",
      "|   BIZ-1019|Reid, Ferguson an...|   Restaurant|Ahmadu Bello Way,...|       Kunle Ali|   8045935572|\n",
      "+-----------+--------------------+-------------+--------------------+----------------+-------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# create business dimension model \n",
    "dim_business = f4f_df_clean.select('business_id', 'business_name', 'business_type', 'business_address', \\\n",
    "                                   'contact_name', 'contact_phone') \\\n",
    "                                   .dropDuplicates([\"business_id\"])\n",
    "\n",
    "dim_business.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "05785521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------+\n",
      "|rider_id|   rider_name|rider_phone|\n",
      "+--------+-------------+-----------+\n",
      "|       0|   Emeka John| 8019196777|\n",
      "|       1|  Aisha Bello| 8089864260|\n",
      "|       2|Tunde Oladipo| 8019121552|\n",
      "|       3| Grace Onyema| 8041568532|\n",
      "+--------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dimension table for rider\n",
    "dim_rider = f4f_df_clean.select('rider_name', 'rider_phone').dropDuplicates()  \\\n",
    "                    .withColumn('rider_id', monotonically_increasing_id()) \\\n",
    "                    .select('rider_id','rider_name', 'rider_phone')\n",
    "\n",
    "dim_rider.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a0ccdea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|flour_type_id|  flour_type|\n",
      "+-------------+------------+\n",
      "|            0| All-purpose|\n",
      "|            1|     Unknown|\n",
      "|            2| Whole Wheat|\n",
      "|            3|Pastry Flour|\n",
      "|            4| Bread Flour|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create flour dimension table\n",
    "\n",
    "dim_flour = f4f_df_clean.select('flour_type').dropDuplicates() \\\n",
    "                    .withColumn('flour_type_id', monotonically_increasing_id()) \\\n",
    "                    .select('flour_type_id', 'flour_type')\n",
    "\n",
    "dim_flour.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ce2af71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+-----------+--------+-------------+-------------+-------------+------------+--------------+------------+\n",
      "|  order_id|order_date|delivery_date|business_id|rider_id|flour_type_id|quantity_bags|price_per_bag|total_amount|payment_method|order_status|\n",
      "+----------+----------+-------------+-----------+--------+-------------+-------------+-------------+------------+--------------+------------+\n",
      "|ORD-214576|2025-10-25|   2025-10-25|   BIZ-1018|       1|            1|           26|         9500|      247000|           POS|   Delivered|\n",
      "|ORD-299448|2025-10-08|   2025-10-08|   BIZ-1006|       2|            4|           27|        10000|      270000|           POS|   Cancelled|\n",
      "|ORD-246991|2025-10-17|   2025-10-17|   BIZ-1052|       2|            3|           21|         9800|      205800| Bank Transfer|   Cancelled|\n",
      "|ORD-392075|2025-10-13|   2025-10-13|   BIZ-1035|       2|            0|           20|        10500|      210000| Bank Transfer|     Pending|\n",
      "|ORD-179046|2025-10-14|   2025-10-14|   BIZ-1039|       0|            4|           40|         9800|      380000|           POS|     Pending|\n",
      "+----------+----------+-------------+-----------+--------+-------------+-------------+-------------+------------+--------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# create fact table\n",
    "\n",
    "fact_orders = f4f_df_clean.join(dim_flour, ['flour_type'], 'left') \\\n",
    "                          .join(dim_rider, ['rider_name', 'rider_phone'], 'left') \\\n",
    "                          .select('order_id', 'order_date', 'delivery_date', 'business_id', 'rider_id', 'flour_type_id', \\\n",
    "                                  'quantity_bags', 'price_per_bag', 'total_amount', 'payment_method', 'order_status')\n",
    "\n",
    "fact_orders.show(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "248386af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.0.1'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd997f66",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1392.parquet.\n: java.util.concurrent.ExecutionException: Boxed Exception\r\n\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolve(Promise.scala:99)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\r\n\tat scala.concurrent.Promise.complete(Promise.scala:57)\r\n\tat scala.concurrent.Promise.complete$(Promise.scala:56)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\r\n\tat scala.concurrent.Promise.failure(Promise.scala:109)\r\n\tat scala.concurrent.Promise.failure$(Promise.scala:109)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:104)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$2(QueryStageExec.scala:333)\r\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\r\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\r\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1773)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolve(Promise.scala:99)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\r\n\t\tat scala.concurrent.Promise.complete(Promise.scala:57)\r\n\t\tat scala.concurrent.Promise.complete$(Promise.scala:56)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\r\n\t\tat scala.concurrent.Promise.failure(Promise.scala:109)\r\n\t\tat scala.concurrent.Promise.failure$(Promise.scala:109)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:104)\r\n\t\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$2(QueryStageExec.scala:333)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1773)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t\t... 1 more\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:46)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:194)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:481)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:322)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:320)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:316)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfact_orders\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclean_data/fact_orders\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\onyek\\Documents\\10Alytics Training\\Flour4fourPyspark\\.venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:2003\u001b[39m, in \u001b[36mDataFrameWriter.parquet\u001b[39m\u001b[34m(self, path, mode, partitionBy, compression)\u001b[39m\n\u001b[32m   2001\u001b[39m     \u001b[38;5;28mself\u001b[39m.partitionBy(partitionBy)\n\u001b[32m   2002\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(compression=compression)\n\u001b[32m-> \u001b[39m\u001b[32m2003\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\onyek\\Documents\\10Alytics Training\\Flour4fourPyspark\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\onyek\\Documents\\10Alytics Training\\Flour4fourPyspark\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\onyek\\Documents\\10Alytics Training\\Flour4fourPyspark\\.venv\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o1392.parquet.\n: java.util.concurrent.ExecutionException: Boxed Exception\r\n\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolve(Promise.scala:99)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\r\n\tat scala.concurrent.Promise.complete(Promise.scala:57)\r\n\tat scala.concurrent.Promise.complete$(Promise.scala:56)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\r\n\tat scala.concurrent.Promise.failure(Promise.scala:109)\r\n\tat scala.concurrent.Promise.failure$(Promise.scala:109)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:104)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$2(QueryStageExec.scala:333)\r\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\r\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\r\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1773)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolve(Promise.scala:99)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\r\n\t\tat scala.concurrent.Promise.complete(Promise.scala:57)\r\n\t\tat scala.concurrent.Promise.complete$(Promise.scala:56)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\r\n\t\tat scala.concurrent.Promise.failure(Promise.scala:109)\r\n\t\tat scala.concurrent.Promise.failure$(Promise.scala:109)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:104)\r\n\t\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$2(QueryStageExec.scala:333)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1773)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t\t... 1 more\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:46)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:194)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:481)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:322)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:320)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:316)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# write dimension + fact tables as Parquet\n",
    "\n",
    "dim_business.write.mode(\"overwrite\").parquet(\"clean_data_parquet/dim_business\")\n",
    "dim_rider.write.mode(\"overwrite\").parquet(\"clean_data_parquet/dim_rider\")\n",
    "dim_flour.write.mode(\"overwrite\").parquet(\"clean_data_parquet/dim_flour\")\n",
    "fact_orders.write.mode(\"overwrite\").parquet(\"clean_data_parquet/fact_orders\")\n",
    "\n",
    "print(\"All DataFrames written to Parquet successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c968cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the transformed data to parquet\n",
    "#transaction.write.mode('overwrite').parquet(r'dataset/transaction')\n",
    "#customer.write.mode('overwrite').parquet(r'dataset/customer')\n",
    "#employee.write.mode('overwrite').parquet(r'dataset/employee')\n",
    "#fact_table.write.mode('overwrite').parquet(r'dataset/fact_table')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0e567ca4",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1388.csv.\n: java.util.concurrent.ExecutionException: Boxed Exception\r\n\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolve(Promise.scala:99)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\r\n\tat scala.concurrent.Promise.complete(Promise.scala:57)\r\n\tat scala.concurrent.Promise.complete$(Promise.scala:56)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\r\n\tat scala.concurrent.Promise.failure(Promise.scala:109)\r\n\tat scala.concurrent.Promise.failure$(Promise.scala:109)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:104)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$2(QueryStageExec.scala:333)\r\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\r\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\r\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1773)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:426)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolve(Promise.scala:99)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\r\n\t\tat scala.concurrent.Promise.complete(Promise.scala:57)\r\n\t\tat scala.concurrent.Promise.complete$(Promise.scala:56)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\r\n\t\tat scala.concurrent.Promise.failure(Promise.scala:109)\r\n\t\tat scala.concurrent.Promise.failure$(Promise.scala:109)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:104)\r\n\t\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$2(QueryStageExec.scala:333)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1773)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t\t... 1 more\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:194)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:481)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:322)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:320)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:316)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdim_business\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclean_data/dim_business\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m dim_rider.coalesce(\u001b[32m1\u001b[39m).write.csv(\u001b[33m\"\u001b[39m\u001b[33mclean_data/dim_rider\u001b[39m\u001b[33m\"\u001b[39m, header=\u001b[38;5;28;01mTrue\u001b[39;00m, mode=\u001b[33m\"\u001b[39m\u001b[33moverwrite\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m dim_flour.coalesce(\u001b[32m1\u001b[39m).write.csv(\u001b[33m\"\u001b[39m\u001b[33mclean_data/dim_flour\u001b[39m\u001b[33m\"\u001b[39m, header=\u001b[38;5;28;01mTrue\u001b[39;00m, mode=\u001b[33m\"\u001b[39m\u001b[33moverwrite\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\onyek\\Documents\\10Alytics Training\\Flour4fourPyspark\\.venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:2146\u001b[39m, in \u001b[36mDataFrameWriter.csv\u001b[39m\u001b[34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[39m\n\u001b[32m   2127\u001b[39m \u001b[38;5;28mself\u001b[39m.mode(mode)\n\u001b[32m   2128\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m   2129\u001b[39m     compression=compression,\n\u001b[32m   2130\u001b[39m     sep=sep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2144\u001b[39m     lineSep=lineSep,\n\u001b[32m   2145\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2146\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\onyek\\Documents\\10Alytics Training\\Flour4fourPyspark\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\onyek\\Documents\\10Alytics Training\\Flour4fourPyspark\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\onyek\\Documents\\10Alytics Training\\Flour4fourPyspark\\.venv\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o1388.csv.\n: java.util.concurrent.ExecutionException: Boxed Exception\r\n\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolve(Promise.scala:99)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\r\n\tat scala.concurrent.Promise.complete(Promise.scala:57)\r\n\tat scala.concurrent.Promise.complete$(Promise.scala:56)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\r\n\tat scala.concurrent.Promise.failure(Promise.scala:109)\r\n\tat scala.concurrent.Promise.failure$(Promise.scala:109)\r\n\tat scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:104)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$2(QueryStageExec.scala:333)\r\n\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\r\n\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\r\n\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1773)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:426)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolve(Promise.scala:99)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\r\n\t\tat scala.concurrent.Promise.complete(Promise.scala:57)\r\n\t\tat scala.concurrent.Promise.complete$(Promise.scala:56)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\r\n\t\tat scala.concurrent.Promise.failure(Promise.scala:109)\r\n\t\tat scala.concurrent.Promise.failure$(Promise.scala:109)\r\n\t\tat scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:104)\r\n\t\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$2(QueryStageExec.scala:333)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:863)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:841)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:510)\r\n\t\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1773)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\t\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t\t... 1 more\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:194)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:481)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:325)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:322)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:320)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:316)\r\n\tat java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1768)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "dim_business.coalesce(1).write.csv(\"clean_data/dim_business\", header=True, mode=\"overwrite\")\n",
    "dim_rider.coalesce(1).write.csv(\"clean_data/dim_rider\", header=True, mode=\"overwrite\")\n",
    "dim_flour.coalesce(1).write.csv(\"clean_data/dim_flour\", header=True, mode=\"overwrite\")\n",
    "fact_orders.coalesce(1).write.csv(\"clean_data/fact_orders\", header=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"All DataFrames exported to CSV successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9cab03",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "226f98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading \n",
    "\n",
    "def get_db_connection():\n",
    "    connection = psycopg2.connect(\n",
    "        host='localhost',\n",
    "        database='F4F_DB',\n",
    "        user='postgres',\n",
    "        password='London123'\n",
    "    )\n",
    "    return connection\n",
    "\n",
    "# connect to sql database\n",
    "conn = get_db_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "268e55d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- delivery_date: date (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- rider_id: long (nullable = true)\n",
      " |-- flour_type_id: long (nullable = true)\n",
      " |-- quantity_bags: integer (nullable = true)\n",
      " |-- price_per_bag: integer (nullable = true)\n",
      " |-- total_amount: integer (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3ac6e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to create tables\n",
    "\n",
    "def create_table():\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    create_table_query = '''\n",
    "   \n",
    "                        CREATE SCHEMA IF NOT EXISTS f4f;\n",
    "\n",
    "                        DROP TABLE IF EXISTS f4f.dim_business CASCADE;\n",
    "                        DROP TABLE IF EXISTS f4f.dim_rider CASCADE;\n",
    "                        DROP TABLE IF EXISTS f4f.dim_flour CASCADE;\n",
    "                        DROP TABLE IF EXISTS f4f.fact_orders CASCADE;\n",
    "\n",
    "                        CREATE TABLE f4f.dim_business (\n",
    "                            business_id VARCHAR PRIMARY KEY,\n",
    "                            business_name VARCHAR NOT NULL,\n",
    "                            business_type VARCHAR,\n",
    "                            business_address VARCHAR,\n",
    "                            contact_name VARCHAR NOT NULL,\n",
    "                            contact_phone VARCHAR NOT NULL\n",
    "                            \n",
    "                        );\n",
    "\n",
    "                        CREATE TABLE f4f.dim_rider (\n",
    "                            rider_id SERIAL PRIMARY KEY,\n",
    "                            rider_name VARCHAR NOT NULL,\n",
    "                            rider_phone VARCHAR NOT NULL\n",
    "                        );\n",
    "\n",
    "                        CREATE TABLE f4f.dim_flour (\n",
    "                            flour_type_id SERIAL PRIMARY KEY,\n",
    "                            flour_type VARCHAR(10000)\n",
    "                            \n",
    "                        );\n",
    "\n",
    "                        CREATE TABLE f4f.fact_orders (\n",
    "                            order_id       VARCHAR PRIMARY KEY,\n",
    "                            order_date     DATE NOT NULL,\n",
    "                            delivery_date  DATE NOT NULL,\n",
    "                            business_id    VARCHAR NOT NULL REFERENCES f4f.dim_business(business_id),\n",
    "                            rider_id       INT NOT NULL REFERENCES f4f.dim_rider(rider_id),\n",
    "                            flour_type_id  INT NOT NULL REFERENCES f4f.dim_flour(flour_type_id),\n",
    "                            quantity_bags  INT NOT NULL,\n",
    "                            price_per_bag   NUMERIC NOT NULL,\n",
    "                            total_amount   NUMERIC NOT NULL,\n",
    "                            payment_method VARCHAR NOT NULL,\n",
    "                            order_status   VARCHAR NOT NULL\n",
    "                        );\n",
    "    '''\n",
    "\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"Tables created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "42e669c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables created successfully.\n"
     ]
    }
   ],
   "source": [
    "create_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5ee0924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data into table\n",
    "\n",
    "url = \"jdbc:postgresql://localhost:5432/F4F_DB\"\n",
    "properties = {\n",
    "    \"user\" : \"postgres\",\n",
    "    \"password\" : \"London123\",\n",
    "    \"driver\" : \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ad8f4603",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_business.write.jdbc(url=url, table=\"f4f.dim_business\", mode=\"append\", properties=properties)\n",
    "dim_rider.write.jdbc(url=url, table=\"f4f.dim_rider\", mode=\"append\", properties=properties)\n",
    "dim_flour.write.jdbc(url=url, table=\"f4f.dim_flour\", mode=\"append\", properties=properties)\n",
    "fact_orders.write.jdbc(url=url, table=\"f4f.fact_orders\", mode=\"append\", properties=properties)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
